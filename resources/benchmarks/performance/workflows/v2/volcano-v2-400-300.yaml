name: volcano-performance-benchmark-v2-400-300
description: |
  Performance benchmark for Volcano with 300 nodes and one large multi-pod job with 400 replicas.
  This tests the scheduler's efficiency when handling a single large workload.
tasks:
  # Configure Volcano
  - id: configure-volcano
    type: Configure
    params:
      configmaps:
        - name: volcano-scheduler-configmap
          namespace: volcano-system
          op: create
          data:
            volcano-scheduler.conf: |
              actions: "enqueue, allocate, backfill"
              tiers:
              - plugins:
                - name: priority
                - name: gang
                - name: conformance
              - plugins:
                - name: drf
                - name: predicates
                - name: proportion
                - name: nodeorder
                - name: binpack
      deploymentRestarts:
        - namespace: volcano-system
          name: volcano-scheduler
      timeout: 10m

  - id: config-sleep
    type: Sleep
    params:
      timeout: 5s

  # Configure nodes
  - id: configure-nodes
    type: Configure
    params:
      nodes:
        - type: hpc.gpu
          count: 300
          labels:
            nvidia.com/gpu.count: "8"
            nvidia.com/mlnxnics: "16"
            nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
          resources:
            cpu: 128
            memory: "1Ti"
            pods: 110
            nvidia.com/gpu: 8
            nvidia.com/mlnxnics: 16
            hugepages-1Gi: 0
            hugepages-2Mi: 0
            ephemeral-storage: "30Ti"
      timeout: 5m

  # Register job template
  - id: register-job
    type: RegisterObj
    params:
      template: "resources/benchmarks/performance/templates/volcano/job.yaml"
      nameFormat: "perftest-v2-400-300-job{{._ENUM_}}"
      podNameFormat: "{{._NAME_}}-test-[0-9]+"
      podCount: "{{.replicas}}"

  # Submit 1 job with 400 replicas
  # CPU = 400 pods × 16 cores (16000m) = 6,400 cores total (16.67% of 38,400)
  # Memory = 400 pods × 256Gi = 102,400Gi = ~100Ti total (33.33% of 300Ti)
  # GPU = 400 pods × 4 GPU = 1,600 GPUs total (66.67% of 2,400)
  - id: job
    type: SubmitObj
    params:
      refTaskId: register-job
      count: 1
      params:
        namespace: default
        minAvailable: 400
        replicas: 400
        queue: default
        image: ubuntu
        cpu: 16000m # 16 cores (12.5% node CPU)
        memory: 256Gi # 256GB (25% node)
        gpu: 4 # 4 GPU (50% node GPU)
        ttl: 5m
