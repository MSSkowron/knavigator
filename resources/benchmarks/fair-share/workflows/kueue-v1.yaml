name: kueue-fair-share-benchmark-v1
description: |
  Fair Share benchmark for Kueue scheduler.

  This benchmark tests whether Kueue properly implements fair resource sharing
  between tenants with equal priority.

  The test creates three namespaces with identical ClusterQueues in a cohort.
  Each queue has identical priority settings and resource quota.
  The test then submits multiple jobs to all tenants and verifies that resources are distributed fairly.

  Test scenario:
  1. Configure cluster with limited CPU resources (75 CPU total)
  2. Set up Kueue with fairSharing enabled
  3. Create three tenants with equal priority
  4. Submit multiple jobs to all tenants
  5. Verify that all tenants receive an equal share of resources
tasks:
  # Configure Kueue
  - id: configure-kueue
    type: Configure
    params:
      configmaps:
        - name: kueue-manager-config
          namespace: kueue-system
          op: create
          data:
            controller_manager_config.yaml: |
              apiVersion: config.kueue.x-k8s.io/v1beta1
              kind: Configuration
              health:
                healthProbeBindAddress: :8081
              metrics:
                bindAddress: :8080
              webhook:
                port: 9443
              leaderElection:
                leaderElect: true
                resourceName: c1f6bfd2.kueue.x-k8s.io
              controller:
                groupKindConcurrency:
                  Job.batch: 5
                  Pod: 5
                  Workload.kueue.x-k8s.io: 5
                  LocalQueue.kueue.x-k8s.io: 1
                  ClusterQueue.kueue.x-k8s.io: 1
                  ResourceFlavor.kueue.x-k8s.io: 1
              clientConnection:
                qps: 50
                burst: 100
              waitForPodsReady:
                enable: true
                timeout: 5m
                blockAdmission: true
                requeuingStrategy:
                  timestamp: Eviction
                  backoffLimitCount: null # null indicates infinite requeuing
                  backoffBaseSeconds: 60
                  backoffMaxSeconds: 3600
              integrations:
                frameworks:
                - "batch/job"
                - "kubeflow.org/mpijob"
                - "ray.io/rayjob"
                - "ray.io/raycluster"
                - "jobset.x-k8s.io/jobset"
                - "kubeflow.org/paddlejob"
                - "kubeflow.org/pytorchjob"
                - "kubeflow.org/tfjob"
                - "kubeflow.org/xgboostjob"
              fairSharing:
               enable: true
               preemptionStrategies: [LessThanOrEqualToFinalShare, LessThanInitialShare]
      deploymentRestarts:
        - namespace: kueue-system
          name: kueue-controller-manager
      timeout: 10m

  - id: config-sleep
    type: Sleep
    params:
      timeout: 5s

  # Configure nodes
  - id: configure-nodes
    type: Configure
    params:
      nodes:
        - type: cpu-node
          count: 5
          resources:
            cpu: 15
            memory: "16Gi"
            pods: 110
      timeout: 5m

  # Create namespaces for the three tenants
  - id: create-namespaces
    type: Configure
    params:
      namespaces:
        - name: tenant-a
          op: create
        - name: tenant-b
          op: create
        - name: tenant-c
          op: create
      timeout: 1m

  # Register Kueue-specific resources
  - id: register-cluster-queue
    type: RegisterObj
    params:
      template: "resources/benchmarks/fair-share/templates/kueue/cluster-queue.yaml"

  - id: register-local-queue
    type: RegisterObj
    params:
      template: "resources/benchmarks/fair-share/templates/kueue/local-queue.yaml"

  - id: register-resource-flavor
    type: RegisterObj
    params:
      template: "resources/benchmarks/fair-share/templates/kueue/resource-flavor.yaml"

  - id: register-namespace
    type: RegisterObj
    params:
      template: "resources/benchmarks/fair-share/templates/k8s/namespace.yaml"

  - id: register-job
    type: RegisterObj
    params:
      template: "resources/benchmarks/fair-share/templates/kueue/job.yaml"
      nameFormat: "fairshare-v1-job{{._ENUM_}}"
      podNameFormat: "{{._NAME_}}-[0-9]-.*"
      podCount: "{{.replicas}}"

  - id: create-resource-flavor
    type: SubmitObj
    params:
      refTaskId: register-resource-flavor
      canExist: true
      params:
        name: "resource-flavor"
        nodeLabels:
          type: "kwok"

  # Create a cohort and three cluster queues with equal weight
  - id: create-cluster-queue-a
    type: SubmitObj
    params:
      refTaskId: register-cluster-queue
      canExist: true
      params:
        name: tenant-a-cq
        cohort: fairshare-cohort # All queues in the same cohort
        fairSharing:
          weight: 1
        resourceGroups:
          - coveredResources:
              - cpu
              - memory
              - pods
            flavors:
              - name: resource-flavor
                resources:
                  - name: cpu
                    nominalQuota: 25 # Gwarantowane 25 CPU (guaranteed)
                    borrowingLimit: 50 # Może pożyczyć dodatkowe 50 CPU
                    lendingLimit: 25 # Może udostępnić do 25 CPU innym
                  - name: memory
                    nominalQuota: 25Gi # Gwarantowane 25Gi pamięci
                    borrowingLimit: 50Gi # Może pożyczyć dodatkowe 50Gi
                    lendingLimit: 25Gi # Może udostępnić do 25Gi innym
                  - name: pods
                    nominalQuota: 183 # 1/3 z łącznej liczby 550 podów
                    borrowingLimit: 367 # 2/3 z łącznej liczby 550 podów
                    lendingLimit: 183 # Maksymalnie może udostępnić swój przydział
        preemption:
          reclaimWithinCohort: Any

  - id: create-cluster-queue-b
    type: SubmitObj
    params:
      refTaskId: register-cluster-queue
      canExist: true
      params:
        name: tenant-b-cq
        cohort: fairshare-cohort # All queues in the same cohort
        fairSharing:
          weight: 1
        resourceGroups:
          - coveredResources:
              - cpu
              - memory
              - pods
            flavors:
              - name: resource-flavor
                resources:
                  - name: cpu
                    nominalQuota: 25 # Gwarantowane 25 CPU (guaranteed)
                    borrowingLimit: 50 # Może pożyczyć dodatkowe 50 CPU
                    lendingLimit: 25 # Może udostępnić do 25 CPU innym
                  - name: memory
                    nominalQuota: 25Gi # Gwarantowane 25Gi pamięci
                    borrowingLimit: 50Gi # Może pożyczyć dodatkowe 50Gi
                    lendingLimit: 25Gi # Może udostępnić do 25Gi innym
                  - name: pods
                    nominalQuota: 183 # 1/3 z łącznej liczby 550 podów
                    borrowingLimit: 367 # 2/3 z łącznej liczby 550 podów
                    lendingLimit: 183 # Maksymalnie może udostępnić swój przydział
        preemption:
          reclaimWithinCohort: Any

  - id: create-cluster-queue-c
    type: SubmitObj
    params:
      refTaskId: register-cluster-queue
      canExist: true
      params:
        name: tenant-c-cq
        cohort: fairshare-cohort # All queues in the same cohort
        fairSharing:
          weight: 1
        resourceGroups:
          - coveredResources:
              - cpu
              - memory
              - pods
            flavors:
              - name: resource-flavor
                resources:
                  - name: cpu
                    nominalQuota: 25 # Gwarantowane 25 CPU (guaranteed)
                    borrowingLimit: 50 # Może pożyczyć dodatkowe 50 CPU
                    lendingLimit: 25 # Może udostępnić do 25 CPU innym
                  - name: memory
                    nominalQuota: 25Gi # Gwarantowane 25Gi pamięci
                    borrowingLimit: 50Gi # Może pożyczyć dodatkowe 50Gi
                    lendingLimit: 25Gi # Może udostępnić do 25Gi innym
                  - name: pods
                    nominalQuota: 183 # 1/3 z łącznej liczby 550 podów
                    borrowingLimit: 367 # 2/3 z łącznej liczby 550 podów
                    lendingLimit: 183 # Maksymalnie może udostępnić swój przydział
        preemption:
          reclaimWithinCohort: Any

  # Create local queues for all tenants
  - id: create-local-queue-a
    type: SubmitObj
    params:
      refTaskId: register-local-queue
      canExist: true
      params:
        name: tenant-a-queue
        namespace: tenant-a
        clusterQueue: tenant-a-cq

  - id: create-local-queue-b
    type: SubmitObj
    params:
      refTaskId: register-local-queue
      canExist: true
      params:
        name: tenant-b-queue
        namespace: tenant-b
        clusterQueue: tenant-b-cq

  - id: create-local-queue-c
    type: SubmitObj
    params:
      refTaskId: register-local-queue
      canExist: true
      params:
        name: tenant-c-queue
        namespace: tenant-c
        clusterQueue: tenant-c-cq

  # Submit jobs for tenant A (requesting 30 CPU)
  - id: submit-jobs-tenant-a
    type: SubmitObj
    params:
      refTaskId: register-job
      count: 30 # Submit 30 jobs
      params:
        namespace: tenant-a
        queueName: tenant-a-queue
        replicas: 1
        completionMode: NonIndexed
        image: ubuntu
        cpu: 1000m # Each job requests 1 CPU
        memory: 1Gi
        ttl: "5m"

  # Submit jobs for tenant B (requesting 30 CPU)
  - id: submit-jobs-tenant-b
    type: SubmitObj
    params:
      refTaskId: register-job
      count: 30 # Submit 30 jobs
      params:
        namespace: tenant-b
        queueName: tenant-b-queue
        replicas: 1
        completionMode: NonIndexed
        image: ubuntu
        cpu: 1000m # Each job requests 1 CPU
        memory: 1Gi
        ttl: "5m"

  # Submit jobs for tenant C (requesting 30 CPU)
  - id: submit-jobs-tenant-c
    type: SubmitObj
    params:
      refTaskId: register-job
      count: 30 # Submit 30 jobs
      params:
        namespace: tenant-c
        queueName: tenant-c-queue
        replicas: 1
        completionMode: NonIndexed
        image: ubuntu
        cpu: 1000m # Each job requests 1 CPU
        memory: 1Gi
        ttl: "5m"

  # # Wait for jobs to be scheduled and start running
  # - id: wait-for-scheduling
  #   type: Sleep
  #   params:
  #     timeout: 30s

  # # Check tenant A has fair share of resources (should be around 25 pods running)
  # - id: check-tenant-a-active
  #   type: CheckPod
  #   params:
  #     refTaskId: submit-jobs-tenant-a
  #     status: Running
  #     timeout: 10s

  # # Check tenant B has fair share of resources (should be around 25 pods running)
  # - id: check-tenant-b-active
  #   type: CheckPod
  #   params:
  #     refTaskId: submit-jobs-tenant-b
  #     status: Running
  #     timeout: 10s

  # # Check tenant C has fair share of resources (should be around 25 pods running)
  # - id: check-tenant-c-active
  #   type: CheckPod
  #   params:
  #     refTaskId: submit-jobs-tenant-c
  #     status: Running
  #     timeout: 10s

  # # Verify that all tenants have approximately equal number of running pods (~25 each)
  # # For now, we can manually verify by checking the logs or using kubectl
  # - id: pause-for-verification
  #   type: Pause
