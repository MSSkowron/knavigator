name: yunikorn-fair-share-benchmark-v1
description: |
  Fair Share benchmark for Apache YuniKorn scheduler.

  This benchmark tests whether YuniKorn properly implements fair resource sharing
  between queues with equal weights. YuniKorn uses a hierarchical queue structure
  with weights to determine fair shares.

  The test sets up three child queues under a parent queue with equal weights,
  submits multiple jobs to all queues, and verifies that resources are
  distributed fairly between them.

  Test scenario:
  1. Configure cluster with limited CPU resources (75 CPU total)
  2. Configure YuniKorn with three queues having equal weights
  3. Submit multiple jobs to all queues (each queue requesting 30 CPU)
  4. Verify that all queues receive an equal share of resources (~25 CPU each)
tasks:
  # Configure YuniKorn
  - id: configure-yunikorn
    type: Configure
    params:
      configmaps:
        - name: yunikorn-configs
          namespace: yunikorn
          op: create
          data:
            queues.yaml: |
              partitions:
                - name: default
                  placementRules:
                    - name: tag
                      value: namespace
                      create: true
                  queues:
                  - name: root
                    submitacl: '*'
                    queues:
                    - name: fairshare
                      submitacl: '*'
                      properties:
                        application.sort.policy: fair
                      queues:
                      - name: tenant-a
                        submitacl: '*'
                        resources:
                          guaranteed:
                            memory: 25Gi
                            vcore: 25000m
                          max:
                            memory: 75Gi
                            vcore: 75000m  # 75 CPU total
                        properties:
                          weight: 1        # Equal weight
                          fairshare.enable: "true"
                      - name: tenant-b
                        submitacl: '*'
                        resources:
                          guaranteed:
                            memory: 25Gi
                            vcore: 25000m
                          max:
                            memory: 75Gi
                            vcore: 75000m  # 75 CPU total
                        properties:
                          weight: 1        # Equal weight
                          fairshare.enable: "true"
                      - name: tenant-c
                        submitacl: '*'
                        resources:
                          guaranteed:
                            memory: 25Gi
                            vcore: 25000m
                          max:
                            memory: 75Gi
                            vcore: 75000m  # 75 CPU total
                        properties:
                          weight: 1        # Equal weight
                          fairshare.enable: "true"
      deploymentRestarts:
        - namespace: yunikorn
          name: yunikorn-scheduler
      timeout: 10m

  - id: config-sleep
    type: Sleep
    params:
      timeout: 5s

  # Configure nodes
  - id: configure-nodes
    type: Configure
    params:
      nodes:
        - type: cpu-node
          count: 5
          resources:
            cpu: 15
            memory: "15Gi"
            pods: 110
      timeout: 5m

  # Register job template
  - id: register-job
    type: RegisterObj
    params:
      template: "resources/benchmarks/fair-share/templates/yunikorn/job.yml"
      nameFormat: "fairshare-v1-job{{._ENUM_}}"
      podNameFormat: "{{._NAME_}}-.*"
      podCount: "{{.replicas}}"

  # Submit jobs for tenant A (requesting 30 CPU)
  - id: submit-jobs-tenant-a
    type: SubmitObj
    params:
      refTaskId: register-job
      count: 30 # Submit 30 jobs
      params:
        namespace: default
        replicas: 1
        minMember: 1
        applicationId: "tenant-a-jobs"
        queue: "root.fairshare.tenant-a"
        image: ubuntu
        cpu: 1000m # Each job requests 1 CPU
        memory: 1Gi
        ttl: "5m"

  # Submit jobs for tenant B (requesting 30 CPU)
  - id: submit-jobs-tenant-b
    type: SubmitObj
    params:
      refTaskId: register-job
      count: 30 # Submit 30 jobs
      params:
        namespace: default
        replicas: 1
        minMember: 1
        applicationId: "tenant-b-jobs"
        queue: "root.fairshare.tenant-b"
        image: ubuntu
        cpu: 1000m # Each job requests 1 CPU
        memory: 1Gi
        ttl: "5m"

  # Submit jobs for tenant C (requesting 30 CPU)
  - id: submit-jobs-tenant-c
    type: SubmitObj
    params:
      refTaskId: register-job
      count: 30 # Submit 30 jobs
      params:
        namespace: default
        replicas: 1
        minMember: 1
        applicationId: "tenant-c-jobs"
        queue: "root.fairshare.tenant-c"
        image: ubuntu
        cpu: 1000m # Each job requests 1 CPU
        memory: 1Gi
        ttl: "5m"

  # # Wait for jobs to be scheduled and start running
  # - id: wait-for-scheduling
  #   type: Sleep
  #   params:
  #     timeout: 30s

  # # Check tenant A has fair share of resources (should be around 25 pods running)
  # - id: check-tenant-a-active
  #   type: CheckPod
  #   params:
  #     refTaskId: submit-jobs-tenant-a
  #     status: Running
  #     timeout: 10s

  # # Check tenant B has fair share of resources (should be around 25 pods running)
  # - id: check-tenant-b-active
  #   type: CheckPod
  #   params:
  #     refTaskId: submit-jobs-tenant-b
  #     status: Running
  #     timeout: 10s

  # # Check tenant C has fair share of resources (should be around 25 pods running)
  # - id: check-tenant-c-active
  #   type: CheckPod
  #   params:
  #     refTaskId: submit-jobs-tenant-c
  #     status: Running
  #     timeout: 10s

  # # Verify that all tenants have approximately equal number of running pods (~25 each)
  # # For now, we can manually verify by checking the logs or using kubectl
  # - id: pause-for-verification
  #   type: Pause
