name: volcano-topology-aware-benchmark-v2
description: |
  Test Volcano's Network Topology Aware Scheduling capabilities with a more complex topology.
  This benchmark verifies Volcano's ability to schedule pods based on network topology
  for optimal performance of distributed workloads.

  The benchmark creates a 21-node cluster with a tree-like network topology:
            ______________ sw31 ________________
           /            /       \               \
       sw21          sw22          sw23          sw24
       /  \          /  \          |    \            \
   sw113  sw114  sw123 sw124     sw133    sw134       sw143
    /|\    /|\    /|\   /|\       /|\       | \  \    | \ \
  n1n2n3 n4n5n6 n7n8n9 n10n11n12 n13n14n15 n16n17n18 n19n20n21
  ^^^^^^   x x    x       x  x         x   x            x

  The configuration marks 8 nodes as unschedulable (marked with 'x').
  The optimal nodes (n1, n2, n3) are all within the same block (sw113) - marked with '^'.
  This setup is designed to test if the scheduler can place pods together on the same
  network block level to minimize inter-pod communication latency.

  IMPORTANT: Before running this benchmark, make sure to deploy Volcano with
  Network Topology Aware Scheduling enabled using the command:
  helm upgrade --install volcano volcano-sh/volcano -n volcano-system --create-namespace --version="1.11.0-network-topology-preview.0"
tasks:
  # Configure Volcano
  - id: configure-volcano
    type: Configure
    params:
      configmaps:
        - name: volcano-scheduler-configmap
          namespace: volcano-system
          op: create
          data:
            volcano-scheduler.conf: |
              actions: "enqueue, allocate, backfill"
              tiers:
              - plugins:
                - name: priority
                - name: gang
                - name: conformance
              - plugins:
                - name: drf
                - name: predicates
                - name: proportion
                - name: nodeorder
                - name: binpack
                  arguments:
                    binpack.weight: 10
                    binpack.resources: nvidia.com/gpu
                    binpack.cpu: 1
                    binpack.memory: 1
                    binpack.resources.nvidia.com/gpu: 2
      deploymentRestarts:
        - namespace: volcano-system
          name: volcano-scheduler
      timeout: 10m

  # Configure nodes with network topology
  - id: configure-nodes
    type: Configure
    params:
      nodes:
        # Block sw113 - all 3 nodes available and optimal
        - type: hpc.gpu
          count: 1
          namePrefix: n1
          labels:
            ta-optimal: "true"
            network.topology.kubernetes.io/block: sw113
            network.topology.kubernetes.io/spine: sw21
            network.topology.kubernetes.io/datacenter: sw31
            nvidia.com/gpu.count: "8"
            nvidia.com/mlnxnics: "16"
            nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
          resources:
            cpu: 256
            memory: "2Ti"
            pods: 110
            nvidia.com/gpu: 8
            nvidia.com/mlnxnics: 16
            hugepages-1Gi: 0
            hugepages-2Mi: 0
            ephemeral-storage: "30Ti"
        - type: hpc.gpu
          count: 1
          namePrefix: n2
          labels:
            ta-optimal: "true"
            network.topology.kubernetes.io/block: sw113
            network.topology.kubernetes.io/spine: sw21
            network.topology.kubernetes.io/datacenter: sw31
            nvidia.com/gpu.count: "8"
            nvidia.com/mlnxnics: "16"
            nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
          resources:
            cpu: 256
            memory: "2Ti"
            pods: 110
            nvidia.com/gpu: 8
            nvidia.com/mlnxnics: 16
            hugepages-1Gi: 0
            hugepages-2Mi: 0
            ephemeral-storage: "30Ti"
        - type: hpc.gpu
          count: 1
          namePrefix: n3
          labels:
            ta-optimal: "true"
            network.topology.kubernetes.io/block: sw113
            network.topology.kubernetes.io/spine: sw21
            network.topology.kubernetes.io/datacenter: sw31
            nvidia.com/gpu.count: "8"
            nvidia.com/mlnxnics: "16"
            nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
          resources:
            cpu: 256
            memory: "2Ti"
            pods: 110
            nvidia.com/gpu: 8
            nvidia.com/mlnxnics: 16
            hugepages-1Gi: 0
            hugepages-2Mi: 0
            ephemeral-storage: "30Ti"

        # Block sw114 - two nodes unschedulable
        - type: hpc.gpu
          count: 1
          namePrefix: n4
          labels:
            network.topology.kubernetes.io/block: sw114
            network.topology.kubernetes.io/spine: sw21
            network.topology.kubernetes.io/datacenter: sw31
            nvidia.com/gpu.count: "8"
            nvidia.com/mlnxnics: "16"
            nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
          resources:
            cpu: 256
            memory: "2Ti"
            pods: 110
            nvidia.com/gpu: 8
            nvidia.com/mlnxnics: 16
            hugepages-1Gi: 0
            hugepages-2Mi: 0
            ephemeral-storage: "30Ti"
        - type: hpc.gpu
          count: 1
          namePrefix: n5
          labels:
            network.topology.kubernetes.io/block: sw114
            network.topology.kubernetes.io/spine: sw21
            network.topology.kubernetes.io/datacenter: sw31
            nvidia.com/gpu.count: "8"
            nvidia.com/mlnxnics: "16"
            nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
          resources:
            cpu: 256
            memory: "2Ti"
            pods: 110
            nvidia.com/gpu: 8
            nvidia.com/mlnxnics: 16
            hugepages-1Gi: 0
            hugepages-2Mi: 0
            ephemeral-storage: "30Ti"
        - type: hpc.gpu
          count: 1
          namePrefix: n6
          labels:
            network.topology.kubernetes.io/block: sw114
            network.topology.kubernetes.io/spine: sw21
            network.topology.kubernetes.io/datacenter: sw31
            nvidia.com/gpu.count: "8"
            nvidia.com/mlnxnics: "16"
            nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
          resources:
            cpu: 256
            memory: "2Ti"
            pods: 110
            nvidia.com/gpu: 8
            nvidia.com/mlnxnics: 16
            hugepages-1Gi: 0
            hugepages-2Mi: 0
            ephemeral-storage: "30Ti"

        # Block sw123 - one node unschedulable
        - type: hpc.gpu
          count: 1
          namePrefix: n7
          labels:
            network.topology.kubernetes.io/block: sw123
            network.topology.kubernetes.io/spine: sw22
            network.topology.kubernetes.io/datacenter: sw31
            nvidia.com/gpu.count: "8"
            nvidia.com/mlnxnics: "16"
            nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
          resources:
            cpu: 256
            memory: "2Ti"
            pods: 110
            nvidia.com/gpu: 8
            nvidia.com/mlnxnics: 16
            hugepages-1Gi: 0
            hugepages-2Mi: 0
            ephemeral-storage: "30Ti"
        - type: hpc.gpu
          count: 1
          namePrefix: n8
          labels:
            network.topology.kubernetes.io/block: sw123
            network.topology.kubernetes.io/spine: sw22
            network.topology.kubernetes.io/datacenter: sw31
            nvidia.com/gpu.count: "8"
            nvidia.com/mlnxnics: "16"
            nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
          resources:
            cpu: 256
            memory: "2Ti"
            pods: 110
            nvidia.com/gpu: 8
            nvidia.com/mlnxnics: 16
            hugepages-1Gi: 0
            hugepages-2Mi: 0
            ephemeral-storage: "30Ti"
        - type: hpc.gpu
          count: 1
          namePrefix: n9
          labels:
            network.topology.kubernetes.io/block: sw123
            network.topology.kubernetes.io/spine: sw22
            network.topology.kubernetes.io/datacenter: sw31
            nvidia.com/gpu.count: "8"
            nvidia.com/mlnxnics: "16"
            nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
          resources:
            cpu: 256
            memory: "2Ti"
            pods: 110
            nvidia.com/gpu: 8
            nvidia.com/mlnxnics: 16
            hugepages-1Gi: 0
            hugepages-2Mi: 0
            ephemeral-storage: "30Ti"

        # Block sw124 - two nodes unschedulable
        - type: hpc.gpu
          count: 1
          namePrefix: n10
          labels:
            network.topology.kubernetes.io/block: sw124
            network.topology.kubernetes.io/spine: sw22
            network.topology.kubernetes.io/datacenter: sw31
            nvidia.com/gpu.count: "8"
            nvidia.com/mlnxnics: "16"
            nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
          resources:
            cpu: 256
            memory: "2Ti"
            pods: 110
            nvidia.com/gpu: 8
            nvidia.com/mlnxnics: 16
            hugepages-1Gi: 0
            hugepages-2Mi: 0
            ephemeral-storage: "30Ti"
        - type: hpc.gpu
          count: 1
          namePrefix: n11
          labels:
            network.topology.kubernetes.io/block: sw124
            network.topology.kubernetes.io/spine: sw22
            network.topology.kubernetes.io/datacenter: sw31
            nvidia.com/gpu.count: "8"
            nvidia.com/mlnxnics: "16"
            nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
          resources:
            cpu: 256
            memory: "2Ti"
            pods: 110
            nvidia.com/gpu: 8
            nvidia.com/mlnxnics: 16
            hugepages-1Gi: 0
            hugepages-2Mi: 0
            ephemeral-storage: "30Ti"
        - type: hpc.gpu
          count: 1
          namePrefix: n12
          labels:
            network.topology.kubernetes.io/block: sw124
            network.topology.kubernetes.io/spine: sw22
            network.topology.kubernetes.io/datacenter: sw31
            nvidia.com/gpu.count: "8"
            nvidia.com/mlnxnics: "16"
            nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
          resources:
            cpu: 256
            memory: "2Ti"
            pods: 110
            nvidia.com/gpu: 8
            nvidia.com/mlnxnics: 16
            hugepages-1Gi: 0
            hugepages-2Mi: 0
            ephemeral-storage: "30Ti"

        # Block sw133 - one node unschedulable
        - type: hpc.gpu
          count: 1
          namePrefix: n13
          labels:
            network.topology.kubernetes.io/block: sw133
            network.topology.kubernetes.io/spine: sw23
            network.topology.kubernetes.io/datacenter: sw31
            nvidia.com/gpu.count: "8"
            nvidia.com/mlnxnics: "16"
            nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
          resources:
            cpu: 256
            memory: "2Ti"
            pods: 110
            nvidia.com/gpu: 8
            nvidia.com/mlnxnics: 16
            hugepages-1Gi: 0
            hugepages-2Mi: 0
            ephemeral-storage: "30Ti"
        - type: hpc.gpu
          count: 1
          namePrefix: n14
          labels:
            network.topology.kubernetes.io/block: sw133
            network.topology.kubernetes.io/spine: sw23
            network.topology.kubernetes.io/datacenter: sw31
            nvidia.com/gpu.count: "8"
            nvidia.com/mlnxnics: "16"
            nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
          resources:
            cpu: 256
            memory: "2Ti"
            pods: 110
            nvidia.com/gpu: 8
            nvidia.com/mlnxnics: 16
            hugepages-1Gi: 0
            hugepages-2Mi: 0
            ephemeral-storage: "30Ti"
        - type: hpc.gpu
          count: 1
          namePrefix: n15
          labels:
            network.topology.kubernetes.io/block: sw133
            network.topology.kubernetes.io/spine: sw23
            network.topology.kubernetes.io/datacenter: sw31
            nvidia.com/gpu.count: "8"
            nvidia.com/mlnxnics: "16"
            nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
          resources:
            cpu: 256
            memory: "2Ti"
            pods: 110
            nvidia.com/gpu: 8
            nvidia.com/mlnxnics: 16
            hugepages-1Gi: 0
            hugepages-2Mi: 0
            ephemeral-storage: "30Ti"

        # Block sw134 - one node unschedulable
        - type: hpc.gpu
          count: 1
          namePrefix: n16
          labels:
            network.topology.kubernetes.io/block: sw134
            network.topology.kubernetes.io/spine: sw23
            network.topology.kubernetes.io/datacenter: sw31
            nvidia.com/gpu.count: "8"
            nvidia.com/mlnxnics: "16"
            nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
          resources:
            cpu: 256
            memory: "2Ti"
            pods: 110
            nvidia.com/gpu: 8
            nvidia.com/mlnxnics: 16
            hugepages-1Gi: 0
            hugepages-2Mi: 0
            ephemeral-storage: "30Ti"
        - type: hpc.gpu
          count: 1
          namePrefix: n17
          labels:
            network.topology.kubernetes.io/block: sw134
            network.topology.kubernetes.io/spine: sw23
            network.topology.kubernetes.io/datacenter: sw31
            nvidia.com/gpu.count: "8"
            nvidia.com/mlnxnics: "16"
            nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
          resources:
            cpu: 256
            memory: "2Ti"
            pods: 110
            nvidia.com/gpu: 8
            nvidia.com/mlnxnics: 16
            hugepages-1Gi: 0
            hugepages-2Mi: 0
            ephemeral-storage: "30Ti"
        - type: hpc.gpu
          count: 1
          namePrefix: n18
          labels:
            network.topology.kubernetes.io/block: sw134
            network.topology.kubernetes.io/spine: sw23
            network.topology.kubernetes.io/datacenter: sw31
            nvidia.com/gpu.count: "8"
            nvidia.com/mlnxnics: "16"
            nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
          resources:
            cpu: 256
            memory: "2Ti"
            pods: 110
            nvidia.com/gpu: 8
            nvidia.com/mlnxnics: 16
            hugepages-1Gi: 0
            hugepages-2Mi: 0
            ephemeral-storage: "30Ti"

        # Block sw143 - one node unschedulable
        - type: hpc.gpu
          count: 1
          namePrefix: n19
          labels:
            network.topology.kubernetes.io/block: sw143
            network.topology.kubernetes.io/spine: sw24
            network.topology.kubernetes.io/datacenter: sw31
            nvidia.com/gpu.count: "8"
            nvidia.com/mlnxnics: "16"
            nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
          resources:
            cpu: 256
            memory: "2Ti"
            pods: 110
            nvidia.com/gpu: 8
            nvidia.com/mlnxnics: 16
            hugepages-1Gi: 0
            hugepages-2Mi: 0
            ephemeral-storage: "30Ti"
        - type: hpc.gpu
          count: 1
          namePrefix: n20
          labels:
            network.topology.kubernetes.io/block: sw143
            network.topology.kubernetes.io/spine: sw24
            network.topology.kubernetes.io/datacenter: sw31
            nvidia.com/gpu.count: "8"
            nvidia.com/mlnxnics: "16"
            nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
          resources:
            cpu: 256
            memory: "2Ti"
            pods: 110
            nvidia.com/gpu: 8
            nvidia.com/mlnxnics: 16
            hugepages-1Gi: 0
            hugepages-2Mi: 0
            ephemeral-storage: "30Ti"
        - type: hpc.gpu
          count: 1
          namePrefix: n21
          labels:
            network.topology.kubernetes.io/block: sw143
            network.topology.kubernetes.io/spine: sw24
            network.topology.kubernetes.io/datacenter: sw31
            nvidia.com/gpu.count: "8"
            nvidia.com/mlnxnics: "16"
            nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
          resources:
            cpu: 256
            memory: "2Ti"
            pods: 110
            nvidia.com/gpu: 8
            nvidia.com/mlnxnics: 16
            hugepages-1Gi: 0
            hugepages-2Mi: 0
            ephemeral-storage: "30Ti"
      timeout: 5m

  # Oznaczenie niektórych węzłów jako zajęte (nieplanowalne)
  - id: update-nodes
    type: UpdateNodes
    params:
      selectors:
        - kubernetes.io/hostname: n5 # Block sw114
        - kubernetes.io/hostname: n6 # Block sw114
        - kubernetes.io/hostname: n8 # Block sw123
        - kubernetes.io/hostname: n11 # Block sw124
        - kubernetes.io/hostname: n12 # Block sw124
        - kubernetes.io/hostname: n15 # Block sw133
        - kubernetes.io/hostname: n16 # Block sw134
        - kubernetes.io/hostname: n20 # Block sw143
      state:
        spec:
          unschedulable: true
      timeout: 2m

  # Define HyperNodes for the network topology
  - id: register-hypernode
    type: RegisterObj
    params:
      template: "resources/benchmarks/topology-aware/templates/volcano/hypernode.yaml"

  # Register job template
  - id: register-job-soft
    type: RegisterObj
    params:
      template: "resources/benchmarks/topology-aware/templates/volcano/job.yaml"
      nameFormat: "volcano-tas-job-soft{{._ENUM_}}"
      podNameFormat: "{{._NAME_}}-test-[0-9]+"
      podCount: "{{.replicas}}"

  - id: register-job-hard
    type: RegisterObj
    params:
      template: "resources/benchmarks/topology-aware/templates/volcano/job.yaml"
      nameFormat: "volcano-tas-job-hard{{._ENUM_}}"
      podNameFormat: "{{._NAME_}}-test-[0-9]+"
      podCount: "{{.replicas}}"

  # Create leaf HyperNodes for blocks
  - id: create-hypernode-sw113
    type: SubmitObj
    params:
      refTaskId: register-hypernode
      params:
        name: sw113
        tier: 1
        members:
          - type: Node
            exactMatch: n1
          - type: Node
            exactMatch: n2
          - type: Node
            exactMatch: n3

  - id: create-hypernode-sw114
    type: SubmitObj
    params:
      refTaskId: register-hypernode
      params:
        name: sw114
        tier: 1
        members:
          - type: Node
            exactMatch: n4
          - type: Node
            exactMatch: n5
          - type: Node
            exactMatch: n6

  - id: create-hypernode-sw123
    type: SubmitObj
    params:
      refTaskId: register-hypernode
      params:
        name: sw123
        tier: 1
        members:
          - type: Node
            exactMatch: n7
          - type: Node
            exactMatch: n8
          - type: Node
            exactMatch: n9

  - id: create-hypernode-sw124
    type: SubmitObj
    params:
      refTaskId: register-hypernode
      params:
        name: sw124
        tier: 1
        members:
          - type: Node
            exactMatch: n10
          - type: Node
            exactMatch: n11
          - type: Node
            exactMatch: n12

  - id: create-hypernode-sw133
    type: SubmitObj
    params:
      refTaskId: register-hypernode
      params:
        name: sw133
        tier: 1
        members:
          - type: Node
            exactMatch: n13
          - type: Node
            exactMatch: n14
          - type: Node
            exactMatch: n15

  - id: create-hypernode-sw134
    type: SubmitObj
    params:
      refTaskId: register-hypernode
      params:
        name: sw134
        tier: 1
        members:
          - type: Node
            exactMatch: n16
          - type: Node
            exactMatch: n17
          - type: Node
            exactMatch: n18

  - id: create-hypernode-sw143
    type: SubmitObj
    params:
      refTaskId: register-hypernode
      params:
        name: sw143
        tier: 1
        members:
          - type: Node
            exactMatch: n19
          - type: Node
            exactMatch: n20
          - type: Node
            exactMatch: n21

  # Create spine layer HyperNodes
  - id: create-hypernode-sw21
    type: SubmitObj
    params:
      refTaskId: register-hypernode
      params:
        name: sw21
        tier: 2
        members:
          - type: HyperNode
            exactMatch: sw113
          - type: HyperNode
            exactMatch: sw114

  - id: create-hypernode-sw22
    type: SubmitObj
    params:
      refTaskId: register-hypernode
      params:
        name: sw22
        tier: 2
        members:
          - type: HyperNode
            exactMatch: sw123
          - type: HyperNode
            exactMatch: sw124

  - id: create-hypernode-sw23
    type: SubmitObj
    params:
      refTaskId: register-hypernode
      params:
        name: sw23
        tier: 2
        members:
          - type: HyperNode
            exactMatch: sw133
          - type: HyperNode
            exactMatch: sw134

  - id: create-hypernode-sw24
    type: SubmitObj
    params:
      refTaskId: register-hypernode
      params:
        name: sw24
        tier: 2
        members:
          - type: HyperNode
            exactMatch: sw143

  # Create datacenter layer HyperNode
  - id: create-hypernode-sw31
    type: SubmitObj
    params:
      refTaskId: register-hypernode
      params:
        name: sw31
        tier: 3
        members:
          - type: HyperNode
            exactMatch: sw21
          - type: HyperNode
            exactMatch: sw22
          - type: HyperNode
            exactMatch: sw23
          - type: HyperNode
            exactMatch: sw24

  # Submit a job with network topology constraints (hard mode)
  - id: job-hard
    type: SubmitObj
    params:
      refTaskId: register-job-hard
      count: 1
      params:
        namespace: default
        replicas: 3
        queue: default
        cpu: 16
        memory: 32Gi
        gpu: 8
        ttl: "2m"
        networkTopology: true
        topologyMode: hard
        highestTierAllowed: 1 # Block level

  # Check if pods are running on the optimal nodes
  - id: check-hard-topology
    type: CheckPod
    description: confirm that the pods initiated by the job with hard topology are running on optimal nodes
    params:
      refTaskId: job-hard
      status: Running
      nodeLabels:
        ta-optimal: "true"
      timeout: 30s

  # Cleanup
  - id: cleanup-job-hard
    type: DeleteObj
    params:
      refTaskId: job-hard

  # Submit a job with network topology constraints (soft mode)
  - id: job-soft
    type: SubmitObj
    params:
      refTaskId: register-job-soft
      count: 1
      params:
        namespace: default
        replicas: 3
        queue: default
        cpu: 16
        memory: 32Gi
        gpu: 8
        ttl: "2m"
        networkTopology: true
        topologyMode: soft
        highestTierAllowed: 1 # Block level

  # Check if pods are running on the optimal nodes
  - id: check-soft-topology
    type: CheckPod
    description: confirm that the pods initiated by the job with soft topology are running on optimal nodes
    params:
      refTaskId: job-soft
      status: Running
      nodeLabels:
        ta-optimal: "true"
      timeout: 30s
